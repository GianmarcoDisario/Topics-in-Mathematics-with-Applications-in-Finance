{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Check out the [Linear Algebra playlist from 3Blue1Brown](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) to get an intuitive feeling for Linear Algebra instead of just knowing the technical details.**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1\n",
    "\n",
    "<style type=\"text/css\">\n",
    "    ol ol { list-style-type: lower-alpha; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem A-1.\n",
    "Decide wheter the following statements are true or false. No explanation needed.\n",
    "\n",
    "1. *The row-rank and column-rank of a matrix are always equal to each other.* **True**\n",
    "2. *For a $m\\times n$ matrix $A$ and $n\\times l$ matrix $B$, it is always true that $\\text{rank}(AB) \\leq \\text{rank}(A)$*. **True**\n",
    "3. *There are matrices whose left-inverse and right-inverse are not equal to each other, i.e., $\\exists A, B, C$ such that $BA = I$ and $AC = I$*. **False**\n",
    "4. *A matrix has full-rank if and only if it is invertible.* **True**\n",
    "5. *All symmetric matrices are invertible.* **False**\n",
    "6. *A matrix is invertible if all eigenvalues are distinct.* **True**\n",
    "7. *A matrix is diagonalizable if and only if it is invertible.* **True**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem A-2.\n",
    "### Problem\n",
    "\n",
    "Compute the row-rank and column-rank of the following matrices.\n",
    "\n",
    "$$ \n",
    "(a) ~ \\begin{bmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 1 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 1 \\end{bmatrix} ~~~~ \n",
    "(b) ~ \\begin{bmatrix} 1 & 1 & 0 & 0 & 0 & 0 \\\\ 1 & 1 & 0 & 3 & 0 & 0 \\\\ 0 & 0 & 2 & 0 & -1 & 0 \\\\ 0 & 0 & 4 & 0 & -2 & 0 \\\\ \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "For this problem I have chosen to use simple [matrix row operation](https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:elementary-matrix-row-operations/a/matrix-row-operations) to quickly indicate the rank of a matrix. We know from Problem A-1 that the row-rank and column-rank of a matrix is always equal to each other. So if we show that either one is equal to a specific number, then immediately the other one is of the same rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\text{(a)}$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 1 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 & 1 \\end{bmatrix}  ~\\sim~ \\begin{bmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & -1 \\\\ 0 & 0 & 1 & 1 & 1 \\end{bmatrix} ~\\sim~\n",
    "\\begin{bmatrix} 1 & 0 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & 0 & -1 \\\\ 0 & 0 & 1 & 1 & 1 \\end{bmatrix}$$\n",
    "\n",
    "The last operation was performed in order to clearly show that the first $3\\times 3$ part of the matrix is the identity matrix which clearly indicates that the row-rank and column rank are equal to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of the matrix of Problem A-2 (a) is: 3\n"
     ]
    }
   ],
   "source": [
    "A2_a = np.array(\n",
    "    [[1,0,1,0,1],\n",
    "    [1,1,1,0,0],\n",
    "    [0,0,1,1,1]]\n",
    ")\n",
    "rank_A2_a = np.linalg.matrix_rank(A2_a)\n",
    "print(f\"Rank of the matrix of Problem A-2 (a) is: {rank_A2_a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\text{(b)}$\n",
    "\n",
    "$$\\begin{array}{cccc}\n",
    "\\begin{bmatrix} 1 & 1 & 0 & 0 & 0 & 0 \\\\ 1 & 1 & 0 & 3 & 0 & 0 \\\\ 0 & 0 & 2 & 0 & -1 & 0 \\\\ 0 & 0 & 4 & 0 & -2 & 0 \\\\ \\end{bmatrix} & \\sim & \\begin{bmatrix} 1 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 3 & 0 & 0 \\\\ 0 & 0 & 2 & 0 & -1 & 0 \\\\ 0 & 0 & 4 & 0 & -2 & 0 \\\\ \\end{bmatrix} & \\sim \\\\ \n",
    "\\begin{bmatrix} 1 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 3 & 0 & 0 \\\\ 0 & 0 & 2 & 0 & -1 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ \\end{bmatrix}  & \\sim & \\begin{bmatrix} 1 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & -\\frac{1}{2} & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ \\end{bmatrix} \\end{array}$$\n",
    "\n",
    "The last operation involved two operation in one step; interchange of row 3 with row 4 as well as scaling the rows. This was done to make it easily visible what the row-rank is. It is shown that the row-rank is 3 (same identity style matrix from column 2 through column 4). The same could have been achieved with column operations, but because rank in the first linear algebra courses are usually shown through going to the *row*-reduced echelon form; it was also the preferred method here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of the matrix of Problem A-2 (b) is: 3\n"
     ]
    }
   ],
   "source": [
    "A2_b = np.array(\n",
    "    [[1,1,0,0,0,0],\n",
    "    [1,1,0,3,0,0],\n",
    "    [0,0,2,0,-1,0],\n",
    "    [0,0,4,0,-2,0]]\n",
    ")\n",
    "rank_A2_b = np.linalg.matrix_rank(A2_b)\n",
    "print(f\"Rank of the matrix of Problem A-2 (b) is: {rank_A2_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem A-3.\n",
    "### Problem\n",
    "Compute the determinant and inverse of the followign matrices.\n",
    "\n",
    "$$ (a) ~ \\begin{bmatrix} 1 & 2 \\\\ 4 & -1 \\end{bmatrix} ~~~~ (b) ~ \\begin{bmatrix} -1 & -2 & 3 \\\\ 1 & 2 & 0 \\\\ 4 & 6 & 3 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "In order to compute the inverse of a matrix, it is required to compute the determinant. As a result, it is only put in bold as a sub-result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\text{(a)}$\n",
    "\n",
    "We use the [classic method](https://mathworld.wolfram.com/MatrixInverse.html) for $2\\times 2$ matrices: For a matrix $A = \\begin{bmatrix} a, b \\\\ c, d \\end{bmatrix}$, the inverse is calculated by: \n",
    "\n",
    "$$A^{-1} =  \\begin{bmatrix} a, b \\\\ c, d \\end{bmatrix}^{-1} = \\frac{1}{\\det(A)} \\begin{bmatrix} d, -b \\\\ -c, a \\end{bmatrix} = \\frac{1}{ ad - bc } \\begin{bmatrix} d, -b \\\\ -c, a \\end{bmatrix}$$\n",
    "\n",
    "Or, for our matrix:\n",
    "\n",
    "$$\\begin{bmatrix} 1 & 2 \\\\ 4 & -1 \\end{bmatrix}^{-1} = \\frac{1}{-9} \\begin{bmatrix} -1 & -2 \\\\ -4 & 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{9} & \\frac{2}{9} \\\\ \\frac{4}{9} & -\\frac{1}{9} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The determinant of the matrix of Problem A-3 (a) is: -9.0\n",
      "The inverse of the matrix of Problem A-3 (a) is:\n",
      "[[ 0.111  0.222]\n",
      " [ 0.444 -0.111]]\n"
     ]
    }
   ],
   "source": [
    "A3_a = np.array(\n",
    "    [[1,2],\n",
    "    [4,-1]]\n",
    ")\n",
    "det_A3_a = round(np.linalg.det(A3_a))\n",
    "inv_A3_a = np.linalg.inv(A3_a)\n",
    "print(f\"The determinant of the matrix of Problem A-3 (a) is: {det_A3_a}\")\n",
    "print(\"The inverse of the matrix of Problem A-3 (a) is:\")\n",
    "print(inv_A3_a.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\text{(b)}$\n",
    "\n",
    "For a $3 \\times 3$ matrix, we use the quickest method by first calculating the [determinant](https://www.chilimath.com/lessons/advanced-algebra/determinant-3x3-matrix/) and subsequently the [adjugate matrix](https://en.wikipedia.org/wiki/Adjugate_matrix#3_×_3_generic_matrix) and multiplying the adjugate matrix with the determinant.\n",
    "\n",
    "We start by calculating determinant through the last column. We have to remember that we take the multiplication with $\\left[\\begin{smallmatrix} + & - & + \\\\ - & + & - \\\\ + & - & +\\end{smallmatrix}\\right]$ into account (where it is implied that $\\det(A) = |A|$, but just easier notation):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\det \\left( \\begin{bmatrix} -1 & -2 & 3 \\\\ 1 & 2 & 0 \\\\ 4 & 6 & 3 \\end{bmatrix} \\right) = + 3 \\left|\\begin{matrix}1 & 2 \\\\ 4 & 6 \\end{matrix} \\right| - 0 \\left|\\begin{matrix} -1 & -2 \\\\ 4 & 6 \\end{matrix} \\right| + 3 \\left|\\begin{matrix}-1 & -2 \\\\ 1 & 2 \\end{matrix} \\right| =  -6 + 0 - 0 = -6$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{adj} \\left( \\begin{bmatrix} -1 & -2 & 3 \\\\ 1 & 2 & 0 \\\\ 4 & 6 & 3 \\end{bmatrix} \\right) = \\begin{bmatrix} + \\left| \\begin{matrix} 2 & 0 \\\\ 6 & 3 \\end{matrix} \\right| & - \\left| \\begin{matrix} -2 & 3 \\\\ 6 & 3 \\end{matrix} \\right|  & + \\left| \\begin{matrix} -2 & 3 \\\\ 2 & 0 \\end{matrix} \\right| \\\\ - \\left| \\begin{matrix} 1 & 0 \\\\ 4 & 3 \\end{matrix} \\right| & +\\left| \\begin{matrix} -1 & 3 \\\\ 4 & 3 \\end{matrix} \\right| & -  \\left| \\begin{matrix} -1 & 3 \\\\ 1 & 0 \\end{matrix} \\right|  \\\\ +  \\left| \\begin{matrix} 1 & 2 \\\\ 4 & 6 \\end{matrix} \\right| & - \\left| \\begin{matrix} -1 & -2 \\\\ 4 & 6 \\end{matrix} \\right| & + \\left| \\begin{matrix} -1 & -2 \\\\ 1 & 2 \\end{matrix} \\right| \\end{bmatrix} = \\begin{bmatrix} 6 & 24 & -6 \\\\ -3 & -15 & 3 \\\\ -2 & -2 & 0 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing that rests is calculating the inverse. Which is combining the determinant and the adjugate in the following way:\n",
    "\n",
    "$$ \\begin{bmatrix} -1 & -2 & 3 \\\\ 1 & 2 & 0 \\\\ 4 & 6 & 3 \\end{bmatrix} ^{-1} = \\frac{1}{-6} \\begin{bmatrix} 6 & 24 & -6 \\\\ -3 & -15 & 3 \\\\ -2 & -2 & 0 \\end{bmatrix} = \n",
    "\\begin{bmatrix} -1 & -4 & 1 \\\\ \\frac{1}{2} & 2\\frac{1}{2} & -\\frac{1}{2} \\\\ \\frac{1}{3} & \\frac{1}{3} & 0 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The determinant of the matrix of Problem A-3 (a) is: -6.0\n",
      "The inverse of the matrix of Problem A-3 (a) is:\n",
      "[[-1.    -4.     1.   ]\n",
      " [ 0.5    2.5   -0.5  ]\n",
      " [ 0.333  0.333  0.   ]]\n"
     ]
    }
   ],
   "source": [
    "A3_b = np.array(\n",
    "    [[-1,-2,3],\n",
    "    [1,2,0],\n",
    "    [4,6,3]]\n",
    ")\n",
    "det_A3_b = round(np.linalg.det(A3_b))\n",
    "inv_A3_b = np.linalg.inv(A3_b)\n",
    "print(f\"The determinant of the matrix of Problem A-3 (a) is: {det_A3_b}\")\n",
    "print(\"The inverse of the matrix of Problem A-3 (a) is:\")\n",
    "print(inv_A3_b.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem A-4\n",
    "### Problem\n",
    "Compute the characteristic polynomial and find the eigenvalues and eigenvectors of the following matrix:\n",
    "$$ \\begin{bmatrix} -3 & 3 & 2 \\\\ 1 & -1 & -2 \\\\ -1 & -3 & 0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Let's call the matrix $A_4 = \\left[\\begin{smallmatrix} -3 & 3 & 2 \\\\ 1 & -1 & -2 \\\\ -1 & -3 & 0 \\end{smallmatrix}\\right]$. The eigenvalues correspond by satisfying $A_4 v_i = \\lambda_i v_i$ where $\\lambda_i$ is the $i$-th eigenvalue, with the corresponding eigenvector $v_i$. We can calculate the eigenvalues by determining $\\left|A_4 - \\lambda I\\right|=0$.\n",
    "\n",
    "$$ \\left|A_4 - \\lambda I\\right|= \\left|\\begin{matrix} -3-\\lambda & 3 & 2 \\\\ 1 & -1-\\lambda & -2 \\\\ -1 & -3 & -\\lambda \\end{matrix}\\right| = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by calculating determinant through the last column (starting at the bottom). We have to remember that we take the multiplication with $\\left[\\begin{smallmatrix} + & - & + \\\\ - & + & - \\\\ + & - & +\\end{smallmatrix}\\right] $ into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{array}{rclc}\n",
    "\\left|A_4 - \\lambda I\\right| &=& 0 \\\\ &=& -\\lambda \\left[ (-1-\\lambda)(-3-\\lambda) - 3\\cdot 1\\right] - (-2) \\left[ (-3-\\lambda)(-3) - 3 \\cdot (-1) \\right] + 2 \\left[ 1 \\cdot (-3) - (-1-\\lambda)(-1) \\right] \\\\\n",
    "&=& -\\lambda \\left( 4 \\lambda + \\lambda^2 \\right) + 2 \\left( 3 \\lambda + 12 \\right) + 2 \\left(-4 - \\lambda \\right) \\\\ & = & - \\lambda^3 - 4 \\lambda^2 + 6 \\lambda + 24 - 8 - 2 \\lambda \\\\\n",
    "&=& -\\lambda^3 - 4 \\lambda^2 + 4 \\lambda + 16 \\end{array} $$\n",
    "\n",
    "Finding the roots of this polynomial is quite straightforward:\n",
    "\n",
    "$$ \\begin{array}{rclc} \n",
    "\\left|A_4 - \\lambda I\\right| & = & \\\\\n",
    "0 & = & -\\lambda^3 - 4 \\lambda^2 + 4 \\lambda + 16 &  \\Leftrightarrow \\\\\n",
    "0 & = & \\lambda^3 + 4 \\lambda^2 - 4 \\lambda - 16  \\\\\n",
    "&=& ( \\lambda - 2) ( \\lambda + 4 ) ( \\lambda + 2)\n",
    "\\end{array} $$\n",
    "\n",
    "So we find the eigenvalues $\\lambda_1 = 2$, $\\lambda_2 = -2$, $\\lambda_3 = -4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now calculate one eigenvector and leave the rest as an exercise for the reader. We'll solve for $\\lambda_1$, or $(A_4 - \\lambda_1 I) v_1 = 0$.\n",
    "\n",
    "$$ \\begin{array}{rclc}\n",
    "\\begin{bmatrix} -3-\\lambda_1 & 3 & 2 \\\\ 1 & -1-\\lambda_1 & -2 \\\\ -1 & -3 & -\\lambda_1 \\end{bmatrix} v_1 &=& 0 \\Leftrightarrow\\\\\n",
    "\\begin{bmatrix} -1 & 3 & 2 \\\\ 1 & -3 & -2 \\\\ -1 & -3 & -2 \\end{bmatrix}v_1 &=& 0 & \\Leftrightarrow \\\\\n",
    "\\begin{bmatrix} -1 & 3 & 2 \\\\ 0 & 0 & 0 \\\\ -1 & -3 & -2 \\end{bmatrix}v_1 &=& 0 & \\Leftrightarrow \\\\\n",
    "\\begin{bmatrix} -1 & 3 & 2 \\\\ 0 & 0 & 0 \\\\ 0 & -6 & -4 \\end{bmatrix}v_1 &=& 0 & \\Leftrightarrow \\\\\n",
    "\\begin{bmatrix} -1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 3 & 2 \\end{bmatrix} \\begin{bmatrix} v_{1,1} \\\\ v_{1,2} \\\\ v_{1,3} \\end{bmatrix} &=& 0 \\end{array}$$\n",
    "\n",
    "From here it is clear that the first entry of $v_1$ should be $0$ and as long as we have that $3 v_{1,2} = -2 v_{1,3}$ then we have a valid eigenvector. In general it is desired that the eigenvectors are normal vectors (length 1), so we normalize it once we have found one. Let's say we take the unnormalized vector $\\tilde{v}_1 = \\begin{bmatrix} 0 & 2 & -3 \\end{bmatrix}^T$, and we normalize it, to obtain:\n",
    "\n",
    "$$ v_1 = \\frac{\\tilde{v}_1}{||\\tilde{v}_1||_2} = \\frac{1}{\\sqrt{13}} \\begin{bmatrix} 0 \\\\ 2 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\frac{2}{\\sqrt{13}} \\\\ -\\frac{3}{\\sqrt{13}} \\end{bmatrix} \\approx \\begin{bmatrix} 0 \\\\ 0.55 \\\\ -0.83 \\end{bmatrix}$$\n",
    "\n",
    "The same approach can now be followed for $v_2$ and $v_3$ to obtain:\n",
    "\n",
    "$$ v_2 \\approx \\begin{bmatrix} 0.95 \\\\ -0.32 \\\\ 0 \\end{bmatrix},~~~ v_3 \\approx \\begin{bmatrix} 0.89 \\\\ 0 \\\\ 0.45 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvalues of problem A4 are:\n",
      "[ 2. -4. -2.]\n",
      "with the corresponding eigenvectors:\n",
      "[[-0.    0.95  0.89]\n",
      " [ 0.55 -0.32 -0.  ]\n",
      " [-0.83  0.    0.45]]\n"
     ]
    }
   ],
   "source": [
    "A4 = np.array(\n",
    "    [[-3, 3, 2],\n",
    "    [1, -1, -2],\n",
    "    [-1, -3, 0]]\n",
    ")\n",
    "\n",
    "lambda_A4, eig_A4 = np.linalg.eig(A4)\n",
    "\n",
    "print(\"The eigenvalues of problem A4 are:\")\n",
    "print(lambda_A4)\n",
    "print(\"with the corresponding eigenvectors:\")\n",
    "print(eig_A4.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem A-5\n",
    "\n",
    "$\\text{(a)}$ Find an orthonormal basis of the subspace $\\mathbb{R}^5$ spanned by $v_1, v_2$ and $v_3$ where $v_1 = \\begin{bmatrix} 1 & 0 & 1 & 0 & 1 \\end{bmatrix}^T$, $v_2 = \\begin{bmatrix} 1 & 1 & 1 & 0 & 0 \\end{bmatrix}^T$, and $v_3 = \\begin{bmatrix} 0 & 0 & 1 & 1 & 1 \\end{bmatrix}^T$, using Gram-Schmidt process.\n",
    "\n",
    "$\\text{(b)}$ Complete the basis found in $\\text{(a)}$, into an orthonormal basis of $\\mathbb{R}^5$.\n",
    "\n",
    "$\\text{(c)}$ Find the matrix $U$ which transforms the basis found in $\\text{(b)}$ into the standard basis of $\\mathbb{R}^5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{(a)}$\n",
    "\n",
    "We'll use the standard notation of the [Gram-Schmidt process](https://en.wikipedia.org/wiki/Gram–Schmidt_process). The idea of the Gram-Schmidt process is that we project the vectors to an orthogonal basis by use of linear operations. Let's say we have two vectors in $\\mathbb{R}^2$; $v_1$ and $v_2$ (not the ones from the exercise). Then we can decompose them into $u_1$ and $u_2$ which are orthogonal. After normalizing them we obtain the normalized vectors $e_1$ and $e_2$ which form the basis for $\\mathbb{R}^2$. This process can be seen in the image below.\n",
    "\n",
    "<img src=\"img/gramschmidt.png\" alt=\"Gram-Schmidt Process\" width=\"600\">\n",
    "\n",
    "The basic operations are that we set $u_1 = v_1$, we then project $v_2$ onto $u_1$. Now it is clear that if we substract the projection of $v_2$ on $u_1$ (notation: $\\text{proj}_{u_1}(v_2)$) from $u_1$, we obtain $u_2$. The last thing that remains is normalizing the resulting vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How the projection operator can be derived:*\n",
    "> The projection operator can be seen as mapping $v_2$ onto $u_1$. This means that the resulting vector is a scaled version of $u_1$. Or we could say $\\text{proj}_{u_1}(v_2) = p = c \\cdot u_1$, where $c$ is a constant. \n",
    ">\n",
    "> You can see that $ u_2 $ is orthogonal to $ u_1 $, so we can say $u_2 = v_2 - p $ and consequently this also means that $u_1^T \\cdot u_2 = 0$. We can substitute $ u_2 $ with $ v_2 - p $ and $p$ with $ c \\cdot u_1 $ such that $u_1^T (v_2 - c \\cdot u_1 ) = 0$. Now it's quite easy to solve this for $c$ (since it's the last unknown. It comes down to $u_1^T v_2 - c u_1^T u_1 = 0$. Rewriting this: $c = \\frac{u_1^T v_2}{u_1^T u_1} = \\frac{\\langle u_1, v_2 \\rangle}{ \\langle u_1, u_1 \\rangle} $. Here $\\langle u, v \\rangle$ indicates the dot-product between vectors. This then gives us the way to calculate a projection:\n",
    "> \n",
    "> $$\\text{proj}_{u}(v) = \\frac{\\langle u, v \\rangle}{ \\langle u, u \\rangle} u$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the exercise, this results in taking the followign steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{array}{rclrcl}\n",
    "u_1 &=& v_1, & e_1 & =&  \\frac{u_1}{||u_1||} \\\\\n",
    "u_2 &=& v_2 - \\text{proj}_{u_1} (v_2), & e_2 & =&  \\frac{u_2}{||u_2||} \\\\\n",
    "u_3 &=& v_3 - \\text{proj}_{u_1} (v_3) - \\text{proj}_{u_2} (v_3), & e_3 & =&  \\frac{u_3}{||u_3||} \\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating this for the actual vectors:\n",
    "\n",
    "$$ \n",
    "\\begin{array}{rclrcl}\n",
    "u_1 &=& \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}, & e_1 &=& \\frac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ 0 \\\\ 1  \\\\ 0 \\\\ 1  \\end{bmatrix}\\\\\n",
    "u_2 &=& \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} - \\frac{\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}^T \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}}{\\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}^T \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}}\\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} - \\frac{2}{3} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{3} \\\\ 1 \\\\ \\frac{1}{3} \\\\ 0 \\\\ -\\frac{2}{3} \\end{bmatrix}, & e_2 &=& \\frac{1}{\\sqrt{15}} \\begin{bmatrix} 1 \\\\ 3 \\\\ 1  \\\\ 0 \\\\ -2 \\end{bmatrix}\\\\\n",
    "u_3 &=& \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} - \\frac{2}{3} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix} - (-\\frac{1}{5})\\cdot \\begin{bmatrix} \\frac{1}{3} \\\\ 1 \\\\ \\frac{1}{3} \\\\ 0 \\\\ -\\frac{2}{3} \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{5} \\\\ \\frac{1}{5} \\\\ \\frac{2}{5} \\\\ 1 \\\\ \\frac{1}{5} \\end{bmatrix}, & e_3 &=& \\frac{1}{2 \\sqrt{10}} \\begin{bmatrix} -3 \\\\ 1 \\\\ 2 \\\\ 5 \\\\ 1 \\end{bmatrix}  \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The definition of the basis for a subspace is as follows:*\n",
    "> Let $\\mathcal{S}$ be a subspace of $\\mathbb{R}^n$. A set $\\{b_1, b_2, \\ldots, b_k\\}$ of vectors in $\\mathbb{R}^n$ is called a **basis** for $\\mathcal{S}$ if the following conditions are satisfied:\n",
    "> 1. $\\text{Span}\\{b_1, b_2, \\ldots, b_k\\} = \\mathcal{S}$\n",
    "> 2. The vectors $b_1, b_2, \\ldots, b_k$ are linearly independent.\n",
    "\n",
    "Since we are looking for an orthonormal basis for a subspace of \\mathbb{R}^5, and the set of vectors $e_1, e_2, e_3$ are orthonormal, it is trivial to see that we fulfill the definition. ([Are orthogonal vectors linearly independent?](https://math.stackexchange.com/questions/409810/orthogonality-and-linear-independence/409816))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_1 = np.array([[1],[0],[1],[0],[1]], dtype=float)\n",
    "v_2 = np.array([[1],[1],[1],[0],[0]], dtype=float)\n",
    "v_3 = np.array([[0],[0],[1],[1],[1]], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_schmidt(vectors):\n",
    "    assert isinstance(vectors, tuple), 'Please provide a tuple of numpy arrays'\n",
    "    u_list, e_list = [], []\n",
    "    for i, v in enumerate(vectors):\n",
    "        u = v.copy()\n",
    "        for j in range(i):\n",
    "            u -= _project(u_list[j], v)\n",
    "        u_list += [u]\n",
    "        e_list += [_normalize(u)]\n",
    "        \n",
    "    return tuple(u_list), tuple(e_list)\n",
    "            \n",
    "\n",
    "def _normalize(v):\n",
    "    return v/np.sqrt(np.dot(v.flatten(), v.flatten()))\n",
    "\n",
    "def _project(u, v):\n",
    "    return np.dot(u.flatten(), v.flatten())/np.dot(u.flatten(), u.flatten()) * u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = (v_1, v_2, v_3)\n",
    "u_123, e_123 = gram_schmidt(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3 (column) vectors of u are:\n",
      "[[ 1.     0.333 -0.6  ]\n",
      " [ 0.     1.     0.2  ]\n",
      " [ 1.     0.333  0.4  ]\n",
      " [ 0.     0.     1.   ]\n",
      " [ 1.    -0.667  0.2  ]]\n",
      "The 3 (column) vectors of e are:\n",
      "[[ 0.577  0.258 -0.474]\n",
      " [ 0.     0.775  0.158]\n",
      " [ 0.577  0.258  0.316]\n",
      " [ 0.     0.     0.791]\n",
      " [ 0.577 -0.516  0.158]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The 3 (column) vectors of u are:\")\n",
    "print(np.round(np.hstack((u_123[0], u_123[1], u_123[2])),3))\n",
    "\n",
    "print(\"The 3 (column) vectors of e are:\")\n",
    "print(np.round(np.hstack((e_123[0], e_123[1], e_123[2])),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{(b)}$\n",
    "\n",
    "If we think of the vectors which span the basis of $\\mathbb{R}^n$ as a matrix and see this matrix as a functional mapping of a vector in $\\mathbb{R}^n$ to a scaled (and perhaps rotated) $\\mathbb{R}^n$, then the null space of this matrix necessarily has to be $\\mathbf{0}$ (because the matrix will have to be full rank). So if we find the null space of the basis of the subspace spanned by $e_1, e_2, e_3$, normalize the vectors which span the null space and add them to the basis of the null space, then we have to complete (orthonormal) basis of $\\mathbb{R}^5$. (If the intuition behind this isn't clear, I would suggest to (re-)watch some videos from the [Linear Algebra playlist from 3Blue1Brown](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab). It's amazing in its clarity of explaining the intuition behind Linear Algebra concepts)\n",
    "\n",
    "Let's put all three vectors as rows in a matrix and find the null space. (and for ease of notation, let's scale them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\tilde{E} = \\begin{array}{rcl}\n",
    "\\begin{bmatrix} \\sqrt{3} e_1^T \\\\ \\sqrt{15} e_2^T \\\\ 2\\sqrt{10} e_3^T \\end{bmatrix} &=& \n",
    "\\begin{bmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 1 & 3 & 1 & 0 & -2 \\\\ -3 & 1 & 2 & 5 & 1 \\end{bmatrix} \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "And if we now calculate $\\tilde{E}x = 0$ and solve it for the free variables of $x$, we find the span of the null space. Let's first put $\\tilde{E}$ in its row-reduced echelon form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{array}{cccccc}\n",
    "\\begin{bmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 1 & 3 & 1 & 0 & -2 \\\\ -3 & 1 & 2 & 5 & 1 \\end{bmatrix} & \\sim &\n",
    "\\begin{bmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 0 & 3 & 0 & 0 & -3 \\\\ -3 & 1 & 2 & 5 & 1 \\end{bmatrix} & \\sim &\n",
    "\\begin{bmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & -1 \\\\ -3 & 1 & 2 & 5 & 1 \\end{bmatrix} & \\sim &\\\\\n",
    "\\begin{bmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & -1 \\\\ -3 & 0 & 2 & 5 & 2 \\end{bmatrix} & \\sim & \n",
    "\\begin{bmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & -1 \\\\ -5 & 0 & 0 & 5 & 0 \\end{bmatrix} & \\sim & \n",
    "\\begin{bmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & -1 \\\\ 1 & 0 & 0 & -1 & 0 \\end{bmatrix} \n",
    "\\end{array}$$\n",
    "\n",
    "We can now quite easily find the vector which finds the full space as:\n",
    "\n",
    "$$\\begin{bmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & -1 \\\\ 1 & 0 & 0 & -1 & 0 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "The system of equations which satisfy this is:\n",
    "\n",
    "$$ \\begin{cases}\n",
    "x_1 & = -x_3 - x_5 = -x_3 - x_2 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 & = x_1 = -x_3 - x_2 \\\\\n",
    "x_5 & = x_2\n",
    "\\end{cases}$$\n",
    "\n",
    "If this is now written in terms of the free variables $x_2$ and $x_3$, we get:\n",
    "\n",
    "$$ \\left\\{ \\underbrace{\\begin{bmatrix}-1 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ 1 \\end{bmatrix}}_{v_4} x_2,  \\underbrace{\\begin{bmatrix}-1 \\\\ 0 \\\\ 1 \\\\ -1 \\\\ 0 \\end{bmatrix}}_{v_5} x_3 \\right\\}$$\n",
    "\n",
    "It is straightforward to see that vectors $v_4$ and $v_5$ are not orthonormal vectors to each other. Since they represent the null space, they must be orthogonal to $e_1, e_2, e_3$. If we apply Gram-Schmidt again, but only to $v_4$ and $v_5$, we obtain an $e_4, e_5$ which can be added to the basis. It follows the method described above, so won't be worked out here. We obtain vectors:\n",
    "\n",
    "$$ u_4 = \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ 1 \\end{bmatrix},~ e_4 = \\begin{bmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\\\ -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{bmatrix},~u_5 = \\begin{bmatrix} -\\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\\\ -\\frac{1}{2} \\\\ -\\frac{1}{2} \\end{bmatrix},~ e_5 = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} -\\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\\\ -\\frac{1}{2} \\\\ -\\frac{1}{2} \\end{bmatrix} $$\n",
    "\n",
    "The orthonormal basis for $\\mathbb{R}^5$ is spanned by $\\{e_1, e_2, e_3, e_4, e_5\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_4 = np.array([[-1],[1],[0],[-1],[1]], dtype=float)\n",
    "v_5 = np.array([[-1],[0],[1],[-1],[0]], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_45, e_45 = gram_schmidt((v_4, v_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The basis of R^5 is spanned by the column vectors of:\n",
      "[[ 0.577  0.258 -0.474 -0.5   -0.354]\n",
      " [ 0.     0.775  0.158  0.5   -0.354]\n",
      " [ 0.577  0.258  0.316  0.     0.707]\n",
      " [ 0.     0.     0.791 -0.5   -0.354]\n",
      " [ 0.577 -0.516  0.158  0.5   -0.354]]\n"
     ]
    }
   ],
   "source": [
    "print('The basis of R^5 is spanned by the column vectors of:')\n",
    "print(np.round(np.hstack((e_123[0], e_123[1], e_123[2], e_45[0], e_45[1])),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{(c)}$\n",
    "\n",
    "In order to find a matrix $U$ which transforms the basis found in $\\text{(b)}$ into the standard basis is quite straightforward. Since the basis is an orthonormal basis, we can set up a matrix $E = \\begin{bmatrix}e_1 & e_2 & e_3 & e_4 & e_5 \\end{bmatrix}$. Matrix $E$ is therefore an orthonormal matrix. By definition, if we multiply $E^TE$ we get the identity matrix (which is the standard basis for the space). Thus we can simply conclude that $U=E^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix U is:\n",
      "[[ 0.58  0.26 -0.47 -0.5  -0.35]\n",
      " [ 0.    0.77  0.16  0.5  -0.35]\n",
      " [ 0.58  0.26  0.32  0.    0.71]\n",
      " [ 0.    0.    0.79 -0.5  -0.35]\n",
      " [ 0.58 -0.52  0.16  0.5  -0.35]]\n",
      "If we multiply the matrix U by the matrix E, we obtain:\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "E = np.hstack((e_123[0], e_123[1], e_123[2], e_45[0], e_45[1]))\n",
    "U = E.T\n",
    "\n",
    "print(\"The matrix U is:\")\n",
    "print(np.round(U.T,2))\n",
    "print(\"If we multiply the matrix U by the matrix E, we obtain:\")\n",
    "print(np.round(np.dot(U, E)).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem A-6\n",
    "\n",
    "Use the singular-value decomposition to express the following matrices in the form $A = U \\Sigma V^T$.\n",
    "\n",
    "$$ \\text{(a)} \\begin{bmatrix} 3 & 1 & 1 \\\\ -1 & 3 & 1 \\end{bmatrix}, ~~~ \\text{(b)} \\begin{bmatrix} 1 & 1 \\\\ 2 & 2 \\end{bmatrix}, ~~~ \\text{(c)} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{(a)}$\n",
    "\n",
    "*Derivation*\n",
    ">As we know that $ A = U \\Sigma V^T$, where $U$ and $V$ are orthonormal matrices, we can make the following derivation:\n",
    "> \n",
    "> $A^TA = V \\Sigma U^T U \\Sigma V^T = V \\Sigma^2 V^T$, which leads to $A^T A V = V \\Sigma^2$. So we know that the eigenvectors of $A^T A$ make up the columns of $V$.\n",
    "> \n",
    "> $AA^T = U \\Sigma V^T V \\Sigma U^T = U \\Sigma^2 U^T$, which leads to $A A^T U = U \\Sigma^2$. So we know that the eigenvectors of $A A^T$ make up the columns of $U$.\n",
    ">\n",
    "> Finally, the square roots of the eigenvalues of $A^T A$ or $A A^T$ are the singular values of $\\Sigma$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have matrix $A = \\begin{bmatrix} 3 & 1 & 1 \\\\ -1 & 3 & 1 \\end{bmatrix} $; we calculate $AA^T$ and $A^T A$ and subsequently determine the eigenvectors of both matrices.\n",
    "\n",
    "$$ A^T A = \\begin{bmatrix} 3 & 1 & 1 \\\\ -1 & 3 & 1 \\end{bmatrix}^T \\begin{bmatrix} 3 & 1 & 1 \\\\ -1 & 3 & 1 \\end{bmatrix} = \\begin{bmatrix} 10 & 0 & 2 \\\\ 0 & 10 & 4 \\\\ 2 & 4 & 2 \\end{bmatrix}  $$.\n",
    "\n",
    "$$ A A^T = \\begin{bmatrix} 3 & 1 & 1 \\\\ -1 & 3 & 1 \\end{bmatrix} \\begin{bmatrix} 3 & 1 & 1 \\\\ -1 & 3 & 1 \\end{bmatrix}^T = \\begin{bmatrix} 11 & 1 \\\\ 1 & 11 \\end{bmatrix} $$.\n",
    "\n",
    "We can now calculate the eigenvalues of these matrices. It is the same calculation as Problem A-4, so it will be skipped here. We calculate $ (A^T A - \\lambda I)v = 0$; or $|A^T A - \\lambda I| = 0$ and solve for the roots of $\\lambda$, where we find $\\lambda_1 = 12, \\lambda_2 = 10, \\lambda_3=0$. And for $A A^T$ we get $\\lambda_1 = 12, \\lambda_2 = 10$ as expected.\n",
    "\n",
    "This means we already have $\\Sigma_A = \\begin{bmatrix} \\sqrt{12} & 0 & 0 \\\\ 0 & \\sqrt{10} & 0 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the vectors of $U$. All that is left to do is find the normalized (right) eigenvectors of $ A A^T$. And for $V$ it is the same, but then the normalized (right) eigenvectors of $A^T A$. In general $V$ is written as $V^T$, so we will immediately transpose it after calculation. If we do all this in the same was as Problem A-4, we find that:\n",
    "\n",
    "$$U_A = \\begin{bmatrix} -\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix},~~~ V_A^T = \\begin{bmatrix} -\\frac{1}{\\sqrt{6}} & -\\frac{\\sqrt{2}}{\\sqrt{3}} & -\\frac{1}{\\sqrt{6}} \\\\ -\\frac{2}{\\sqrt{5}} & \\frac{\\sqrt{1}}{\\sqrt{5}} & 0 \\\\ -\\frac{\\sqrt{1}}{\\sqrt{30}} & -\\frac{2}{\\sqrt{30}} & \\frac{\\sqrt{5}}{\\sqrt{6}}\\end{bmatrix}$$\n",
    "\n",
    "This means we get:\n",
    "\n",
    "$$ A = U_A \\Sigma_A V_A^T = \\begin{bmatrix} -\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix} \\begin{bmatrix} \\sqrt{12} & 0 & 0 \\\\ 0 & \\sqrt{10} & 0 \\end{bmatrix}\\begin{bmatrix} -\\frac{1}{\\sqrt{6}} & -\\frac{\\sqrt{2}}{\\sqrt{3}} & -\\frac{1}{\\sqrt{6}} \\\\ -\\frac{2}{\\sqrt{5}} & \\frac{\\sqrt{1}}{\\sqrt{5}} & 0 \\\\ -\\frac{\\sqrt{1}}{\\sqrt{30}} & -\\frac{2}{\\sqrt{30}} & \\frac{\\sqrt{5}}{\\sqrt{6}}\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For $\\text{(b)}$ and $\\text{(c)}$, I won't walk you through the calculation, but simply state the results.**\n",
    "\n",
    "### $\\text{(b)}$\n",
    "$$ B = U_B \\Sigma_B V_B^T = \\begin{bmatrix} -\\frac{\\sqrt{2}}{\\sqrt{5}} & -\\frac{2}{\\sqrt{5}} \\\\ -\\frac{2}{\\sqrt{5}} & \\frac{\\sqrt{2}}{\\sqrt{5}} \\end{bmatrix} \n",
    "\\begin{bmatrix} \\sqrt{10} & 0 \\\\ 0 & 0 \\end{bmatrix}\n",
    "\\begin{bmatrix} -\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{(c)}$\n",
    "$$ C = U_C \\Sigma_C V_C^T = \\begin{bmatrix} -\\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}} \\\\ 0 & 1 & 0 \\\\ -\\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\end{bmatrix} \n",
    "\\begin{bmatrix} 2 & 0 & 0 \\\\ 0&1&0 \\\\ 0&0&0 \\end{bmatrix}\n",
    "\\begin{bmatrix} -\\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}} \\\\ 0 & 1 & 0 \\\\ \\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3, 1, 1],[-1, 3, 1]])\n",
    "B = np.array([[1, 1],[2, 2]])\n",
    "C = np.array([[1, 0, 1],[0, 1, 0],[1, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_A, S_A, Vt_A = np.linalg.svd(A)\n",
    "U_B, S_B, Vt_B = np.linalg.svd(B)\n",
    "U_C, S_C, Vt_C = np.linalg.svd(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will only show the singular values. If they are correct, the rest follows relatively fast.\n",
      "   (a) The singular values of A are the square roots of: [12 10];\n",
      "   (b) The singular values of B are the square roots of: [10  0];\n",
      "   (c) The singular values of C are: [2 1 0].\n"
     ]
    }
   ],
   "source": [
    "print(\"We will only show the singular values. If they are correct, the rest follows relatively fast.\")\n",
    "print(f\"   (a) The singular values of A are the square roots of: {np.round(S_A**2).astype(int)};\")\n",
    "print(f\"   (b) The singular values of B are the square roots of: {np.round(S_B**2).astype(int)};\")\n",
    "print(f\"   (c) The singular values of C are: {np.round(S_C).astype(int)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
